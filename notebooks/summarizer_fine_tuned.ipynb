{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c244545c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4059f4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets peft accelerate sentencepiece evaluate scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39d1b763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>app_name</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>Counter-Strike</td>\n",
       "      <td>Counter-Strike is widely regarded as a classic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>Team Fortress Classic</td>\n",
       "      <td>Team Fortress Classic is regarded by many as a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>Day of Defeat</td>\n",
       "      <td>Reviewers generally find Day of Defeat to be a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>Deathmatch Classic</td>\n",
       "      <td>Deathmatch Classic is widely regarded as a fai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>Half-Life: Opposing Force</td>\n",
       "      <td>Half-Life: Opposing Force receives mixed revie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   app_id                   app_name  \\\n",
       "0      10             Counter-Strike   \n",
       "1      20      Team Fortress Classic   \n",
       "2      30              Day of Defeat   \n",
       "3      40         Deathmatch Classic   \n",
       "4      50  Half-Life: Opposing Force   \n",
       "\n",
       "                                             summary  \n",
       "0  Counter-Strike is widely regarded as a classic...  \n",
       "1  Team Fortress Classic is regarded by many as a...  \n",
       "2  Reviewers generally find Day of Defeat to be a...  \n",
       "3  Deathmatch Classic is widely regarded as a fai...  \n",
       "4  Half-Life: Opposing Force receives mixed revie...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = \"../data/processed/summary_labeled.csv\"  # change if needed\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df = df.dropna(subset=[\"summary\"])\n",
    "\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb7bdeb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['app_id', 'app_name', 'summary'],\n",
       "        num_rows: 45\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['app_id', 'app_name', 'summary'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "test_ds  = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_ds,\n",
    "    \"test\": test_ds,\n",
    "})\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cbddf84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4325ef01f8eb49c1844d3fb4b5e72bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\NLP Steam Review\\Steam-Review-NLP-Pipeline\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ASUS\\.cache\\huggingface\\hub\\models--google--flan-t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d7d0bd082d46c3b315acc7666fa8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2701d99081e54c5289b8eb30b76c9940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d4b8185ae142978eb4e0dcbb51dadb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e36443a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00754444be7a457ea445eaf6dd928dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\NLP Steam Review\\Steam-Review-NLP-Pipeline\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58852b633ca84fa2940d99bed8308363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 45\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_INPUT = 512\n",
    "MAX_TARGET = 160\n",
    "\n",
    "def preprocess_batch(batch):\n",
    "    inputs = [\"summarize reviews: \" + text for text in batch[\"summary\"]]\n",
    "    targets = batch[\"summary\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        truncation=True,\n",
    "        max_length=MAX_INPUT,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            truncation=True,\n",
    "            max_length=MAX_TARGET,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized = dataset.map(\n",
    "    preprocess_batch,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d3b506d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca97be3f28714776829a0fe74e0eeda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203031f0c82141e7bf43456f5677b2e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec4e65fc63a47a6a367a76c32f137dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 688,128 || all params: 77,649,280 || trainable%: 0.8862\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f727f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    refs  = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    preds = [p.strip() for p in preds]\n",
    "    refs  = [r.strip() for r in refs]\n",
    "\n",
    "    result = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "    return {k: round(v * 100, 2) for k, v in result.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8203359a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57.1\n",
      "Transformers version: 4.57.1\n",
      "Transformers module path: d:\\Projects\\NLP Steam Review\\Steam-Review-NLP-Pipeline\\venv\\Lib\\site-packages\\transformers\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "print(\"Transformers module path:\", transformers.__file__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "872f7cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\NLP Steam Review\\Steam-Review-NLP-Pipeline\\venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "852c3a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_33236\\3862147770.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"models/t5_lora_steam\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    # NOTE: no evaluation_strategy, no save_strategy, no predict_with_generate here\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],  # optional, for manual eval later\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    # no compute_metrics — keep it simple for now\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9cf975c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\NLP Steam Review\\Steam-Review-NLP-Pipeline\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 01:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: b8bc61e2-96a0-40af-b6d4-33aaf868f7c5)')' thrown while requesting HEAD https://huggingface.co/google/flan-t5-small/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=36, training_loss=2.0004725986056857, metrics={'train_runtime': 117.5928, 'train_samples_per_second': 1.148, 'train_steps_per_second': 0.306, 'total_flos': 25380598579200.0, 'train_loss': 2.0004725986056857, 'epoch': 3.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4aa03d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LoRA adapter + tokenizer to: ../models/t5_lora_steam\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = \"../models/t5_lora_steam\"\n",
    "\n",
    "model.save_pretrained(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "\n",
    "print(\"Saved LoRA adapter + tokenizer to:\", SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a749c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lora_prompt(all_reviews: str, app_name: str):\n",
    "    prompt = (\n",
    "        f\"You are summarizing community opinions about the game '{app_name}'. \"\n",
    "        f\"Below is a collection of user reviews. \"\n",
    "        f\"Your goal is to write a balanced, neutral summary that captures the common themes across multiple reviews.\\n\\n\"\n",
    "\n",
    "        f\"Instructions:\\n\"\n",
    "        f\"- Focus on overall player sentiment, not a single review.\\n\"\n",
    "        f\"- Highlight gameplay, controls, pacing, difficulty, graphics, performance, audio, and overall enjoyment *only if mentioned*.\\n\"\n",
    "        f\"- If the reviews contradict each other, acknowledge both sides briefly.\\n\"\n",
    "        f\"- If reviews are very short or low-quality, provide the most reasonable interpretation.\\n\"\n",
    "        f\"- Do NOT copy or paraphrase any single review.\\n\"\n",
    "        f\"- Do NOT include slang, insults, or emotional rants.\\n\"\n",
    "        f\"- Do NOT invent details.\\n\"\n",
    "        f\"- Keep the tone calm, factual, and third-person.\\n\"\n",
    "        f\"- Write 2–4 sentences.\\n\\n\"\n",
    "\n",
    "        f\"User Reviews:\\n{all_reviews}\\n\\n\"\n",
    "        f\"Summary:\"\n",
    "    )\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fcd7e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n",
      "Loading tokenizer + LoRA adapter...\n",
      "Generating fine-tuned summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 400/8067 [26:28<6:51:45,  3.22s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial results at row 400 → ../data/processed/fine_lora_partial_400.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 800/8067 [48:34<6:15:00,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial results at row 800 → ../data/processed/fine_lora_partial_800.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1200/8067 [1:09:44<6:45:32,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial results at row 1200 → ../data/processed/fine_lora_partial_1200.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 1600/8067 [1:32:37<5:38:55,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial results at row 1600 → ../data/processed/fine_lora_partial_1600.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 2000/8067 [1:54:54<5:55:10,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial results at row 2000 → ../data/processed/fine_lora_partial_2000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 2400/8067 [2:16:57<5:00:56,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial results at row 2400 → ../data/processed/fine_lora_partial_2400.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 2800/8067 [2:39:13<4:50:46,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial results at row 2800 → ../data/processed/fine_lora_partial_2800.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 3200/8067 [3:01:44<4:43:39,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial results at row 3200 → ../data/processed/fine_lora_partial_3200.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 3600/8067 [3:23:52<3:26:26,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial results at row 3600 → ../data/processed/fine_lora_partial_3600.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 4000/8067 [3:46:14<4:04:10,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial results at row 4000 → ../data/processed/fine_lora_partial_4000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 4400/8067 [4:07:55<3:35:17,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial results at row 4400 → ../data/processed/fine_lora_partial_4400.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 4800/8067 [4:29:42<3:24:08,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial results at row 4800 → ../data/processed/fine_lora_partial_4800.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 5200/8067 [4:51:06<2:45:23,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial results at row 5200 → ../data/processed/fine_lora_partial_5200.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 5600/8067 [5:13:00<2:23:34,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial results at row 5600 → ../data/processed/fine_lora_partial_5600.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 6000/8067 [5:34:31<1:51:57,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial results at row 6000 → ../data/processed/fine_lora_partial_6000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 6400/8067 [5:56:06<1:22:20,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial results at row 6400 → ../data/processed/fine_lora_partial_6400.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 6800/8067 [6:17:47<1:03:25,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial results at row 6800 → ../data/processed/fine_lora_partial_6800.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 7200/8067 [6:37:59<34:14,  2.37s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial results at row 7200 → ../data/processed/fine_lora_partial_7200.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 7600/8067 [6:57:44<21:42,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial results at row 7600 → ../data/processed/fine_lora_partial_7600.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 8000/8067 [7:16:54<03:06,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved partial results at row 8000 → ../data/processed/fine_lora_partial_8000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8067/8067 [7:19:55<00:00,  3.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fine-tuned summaries to: ../data/processed/final_summary_finetuned2.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# paths\n",
    "BASE_MODEL = \"google/flan-t5-small\"\n",
    "ADAPTER_DIR = \"../models/t5_lora_steam\"   # where LoRA weights were saved\n",
    "INPUT_CSV = \"../data/processed/grouped_summary.csv\"  # your original grouped review dataset\n",
    "OUTPUT_CSV = \"../data/processed/final_summary_finetuned2.csv\"\n",
    "\n",
    "# Select range\n",
    "START_IDX = 0\n",
    "END_IDX = len(df)   # Change to len(df) to do all\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# load base model + LoRA adapter\n",
    "print(\"Loading base model...\")\n",
    "base = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL).to(device)\n",
    "\n",
    "print(\"Loading tokenizer + LoRA adapter...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(ADAPTER_DIR)\n",
    "model = PeftModel.from_pretrained(base, ADAPTER_DIR).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# load review data\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "subset = df.iloc[START_IDX:END_IDX].copy()\n",
    "\n",
    "def lora_summarize(all_reviews: str, app_name: str):\n",
    "    # Build prompt\n",
    "    prompt = build_lora_prompt(all_reviews, app_name)\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=160,\n",
    "            num_beams=4,\n",
    "            do_sample=False,\n",
    "            no_repeat_ngram_size=3,\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "print(\"Generating fine-tuned summaries...\")\n",
    "summaries = []\n",
    "\n",
    "for idx, row in tqdm(subset.iterrows(), total=len(subset)):\n",
    "    all_reviews = str(row[\"all_reviews\"])\n",
    "    app_name = str(row[\"app_name\"])\n",
    "    summaries.append(lora_summarize(all_reviews, app_name))\n",
    "    # Save progress every 100 rows\n",
    "    if (idx + 1) % 400 == 0:\n",
    "        subset_partial = subset.iloc[:idx+1].copy()\n",
    "        subset_partial[\"summary_finetuned\"] = summaries\n",
    "        final_subset = subset_partial[[\"app_id\", \"app_name\", \"summary_finetuned\"]].copy()\n",
    "\n",
    "        out_path = f\"../data/processed/fine_lora_partial_{idx+1}.csv\"\n",
    "        final_subset.to_csv(out_path, index=False)\n",
    "        print(f\"Saved partial results at row {idx+1} → {out_path}\")\n",
    "       \n",
    "\n",
    "subset[\"summary_finetuned\"] = summaries\n",
    "\n",
    "subset.to_csv(OUTPUT_CSV, index=False)\n",
    "print(\"Saved fine-tuned summaries to:\", OUTPUT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d39032d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fine-tuned summaries to: ../data/processed/final_summary_finetuned2.csv\n"
     ]
    }
   ],
   "source": [
    "final_subset = subset[[\"app_id\", \"app_name\", \"summary_finetuned\"]].copy()\n",
    "final_subset.to_csv(OUTPUT_CSV, index=False)\n",
    "print(\"Saved fine-tuned summaries to:\", OUTPUT_CSV)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
